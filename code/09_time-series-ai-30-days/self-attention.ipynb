{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention in Time Series Models\n",
    "\n",
    "### 1. Initial Embedding\n",
    "\n",
    "We take each time series, slice it into patches of a fixed length of time steps (32 in this example). \n",
    "\n",
    "These are converted into a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View an illustrated walkthrough here: https://bitsofchris.com/p/how-to-implement-factorized-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# For 4 stocks with 10 time patches each into a 128 dimension embedding space\n",
    "embeddings = torch.randn(4, 10, 128)  # Shape: [4, 10, 128]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Query, Key, Value Projections\n",
    "\n",
    "Our Transformer model is initialized with a Query, Key, and Value matrix in each attention head. These matrices are updated throughout the model training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrices\n",
    "W_query = torch.randn(128, 64)  # embedding_dim -> query_dim\n",
    "W_key = torch.randn(128, 64)    # embedding_dim -> key_dim\n",
    "W_value = torch.randn(128, 128)  # embedding_dim -> value_dim\n",
    "\n",
    "# Project embeddings to Q, K, V for each vector\n",
    "Q = torch.matmul(embeddings, W_query)  # Shape: [4, 10, 64]\n",
    "K = torch.matmul(embeddings, W_key)    # Shape: [4, 10, 64]\n",
    "V = torch.matmul(embeddings, W_value)  # Shape: [4, 10, 128]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Attention Scores and Weights\n",
    "\n",
    "Now we compute how similar a query for a token (what am i looking for) is with the key of every other token (what info I offer).\n",
    "\n",
    "This gives us a set of weights that shows how much a token cares about another token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose K for matrix multiplication\n",
    "K_transposed = K.transpose(-2, -1)  # Shape: [4, 64, 10]\n",
    "\n",
    "# Compute attention scores\n",
    "# Q @ K_T shape: [4, 10, 10]\n",
    "attention_scores = torch.matmul(Q, K_transposed)\n",
    "\n",
    "# Scale scores by square root of key dimension\n",
    "attention_scores = attention_scores / (64 ** 0.5)\n",
    "\n",
    "# Apply softmax to get weights that sum to 1\n",
    "attention_weights = torch.softmax(attention_scores, dim=-1)  # Sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Context Vector Creation\n",
    "\n",
    "Finally, we use these weights to create a weighted sum of values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted sum creates the context vectors\n",
    "# Shape: [4, 10, 128]\n",
    "context_vectors = torch.matmul(attention_weights, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 128])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1.2767, -11.7497,  21.9428,  ...,  -4.2383,  -7.0721,   5.4788],\n",
       "        [  3.9176, -12.2673,   3.2509,  ..., -32.1171,   3.0273,   3.6007],\n",
       "        [ -4.3467,   4.1053,  -6.2469,  ...,  -7.1524,  -3.2380,  -4.0690],\n",
       "        ...,\n",
       "        [ -6.1965,  -9.0071,  -5.2099,  ...,   3.4394,   8.5367,  -2.7584],\n",
       "        [ -9.0476,   7.8684,  10.4539,  ...,   3.8208,  -3.9376,  -1.5432],\n",
       "        [  1.2767, -11.7497,  21.9428,  ...,  -4.2383,  -7.0721,   5.4788]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for one of the stocks\n",
    "context_vectors[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factorized Attention: Splitting Time and Space\n",
    "\n",
    "The above is how attention is calculated in basic self-attention, common for a large language model. In time series, instead of doing one big attention calculation over all dimensions, we split it into time and space dimensions.\n",
    "\n",
    "The time-wise attention is similar to what a LLM does, looking at sequences of patches for one series.\n",
    "\n",
    "Space-wise attention looks at patches across different but related time series at the same time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = torch.randn(4, 10, 128)  # [batch_size=1, stocks, time_patches, embedding_dim]\n",
    "\n",
    "time_embeddings = embeddings.reshape(4, 10, 128) # [batch_size=1 * stocks, time_patches, embedding_dim]\n",
    "\n",
    "# Initialize time-wise projection matrices\n",
    "W_time_query = torch.randn(128, 64)  # embedding_dim -> query_dim\n",
    "W_time_key = torch.randn(128, 64)    # embedding_dim -> key_dim\n",
    "W_time_value = torch.randn(128, 128)  # embedding_dim -> value_dim\n",
    "\n",
    "# Project to Q, K, V\n",
    "time_Q = torch.matmul(time_embeddings, W_time_query)\n",
    "time_K = torch.matmul(time_embeddings, W_time_key)\n",
    "time_V = torch.matmul(time_embeddings, W_time_value)\n",
    "\n",
    "# Compute time-wise attention\n",
    "time_scores = torch.matmul(time_Q, time_K.transpose(-2, -1)) / (64 ** 0.5)\n",
    "time_weights = torch.softmax(time_scores, dim=-1)\n",
    "\n",
    "# Get the context vectors\n",
    "time_context = torch.matmul(time_weights, time_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 128])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_context.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for space-wise attention\n",
    "space_embeddings = time_context.transpose(0, 1)  # shape: [10, 4, 128]\n",
    "\n",
    "W_space_query = torch.randn(128, 64)   \n",
    "W_space_key   = torch.randn(128, 64)   \n",
    "W_space_value = torch.randn(128, 128) \n",
    "\n",
    "# space_embeddings.shape = [10, 4, 128]\n",
    "space_Q = torch.matmul(space_embeddings, W_space_query)\n",
    "space_K = torch.matmul(space_embeddings, W_space_key)   \n",
    "space_V = torch.matmul(space_embeddings, W_space_value)\n",
    "\n",
    "# Compute the space-waise attention\n",
    "space_scores = torch.matmul(space_Q, space_K.transpose(-2, -1)) / (64.0 ** 0.5)\n",
    "space_weights = torch.softmax(space_scores, dim=-1)  # shape: [10, 4, 4]\n",
    "\n",
    "# Weighted sum over values => [10, 4, 128]\n",
    "space_context = torch.matmul(space_weights, space_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 4, 128])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space_context.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "# shape: [10, 4, 128] -> [4, 10, 128]\n",
    "space_context = space_context.transpose(0, 1)\n",
    "print(space_context.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
