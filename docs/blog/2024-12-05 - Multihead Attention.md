# 2024-12-05 - Multihead Attention
Mostly wrapping up the chapter on attention - this was pretty intense, especially today.

But I definitely - having worked through the code and got sidetracked asking questions on tensors, transposing, matrix multiplication, and other PyTorch functions like register_buffer() - have a strong understanding of what's happening.

I can't implement this from scratch on my own, and to be honest - it's probably not worth my time to do that. But having struggled with the fundamentals of attention for almost two weeks now - I get what's happening.

I don't fully understand how we train those QKV matrices, but I think that's coming in a next chapter.

### Next
I'll put the code down for now and finish re-reading the chapter in the book. After that I think it's time for me to create a video recapping what I learned about self-attention. I think will be two parts - the high level concepts and how it clicked for me + a code walk through where I recreate the exercises from Sebastians book in my own words (with as little help from notes as possible).
- [ ] Re-Reach Chapter 3
- [ ] Make a walk through recapping concepts learned & coding self-attention in a notebook